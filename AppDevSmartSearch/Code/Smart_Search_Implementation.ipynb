{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "DB_TYPE = config.DB_TYPE\n",
    "DB_COLS = config.DB_COLS\n",
    "DB_SUBGROUP_COLS = config.DB_SUBGROUP_COLS\n",
    "\n",
    "TECH_WORDS = config.TECH_WORDS\n",
    "DICT_TYPE = config.DICT_TYPEimport config\n",
    "\n",
    "DB_TYPE = config.DB_TYPE\n",
    "DB_COLS = config.DB_COLS\n",
    "DB_SUBGROUP_COLS = config.DB_SUBGROUP_COLS\n",
    "\n",
    "TECH_WORDS = config.TECH_WORDS\n",
    "DICT_TYPE = config.DICT_TYPE\n",
    "TECH_DICT_COLS = config.TECH_DICT_COLS\n",
    "TECH_DICT_TERM_COL = config.TECH_DICT_TERM_COL\n",
    "TECH_DICT_DEFN_COL = config.TECH_DICT_DEFN_COL\n",
    "\n",
    "REGEX_TECHNICAL_WORDS = config.REGEX_TECHNICAL_WORDS\n",
    "TECH_WORDS_REGEX = config.TECH_WORDS_REGEX\n",
    "\n",
    "CSV_PATH_DB = config.CSV_PATH_DB\n",
    "CSV_PATH_DICT = config.CSV_PATH_DICT\n",
    "\n",
    "XLSX_PATH_DB = config.XLSX_PATH_DB\n",
    "XLSX_PATH_DICT = config.XLSX_PATH_DICT\n",
    "\n",
    "MY_SQL_PATH_DB = config.MY_SQL_PATH_DB\n",
    "MY_SQL_USER_DB = config.MY_SQL_USER_DB\n",
    "MY_SQL_PASSWORD_DB = config.MY_SQL_PASSWORD_DB\n",
    "MY_SQL_DB_COLUMNS_QUERY = config.MY_SQL_DB_COLUMNS_QUERY\n",
    "\n",
    "MY_SQL_PATH_DICT = config.MY_SQL_PATH_DICT\n",
    "MY_SQL_USER_DICT = config.MY_SQL_USER_DICT\n",
    "MY_SQL_PASSWORD_DICT = config.MY_SQL_PASSWORD_DICT\n",
    "MY_SQL_DICT_COLUMNS_QUERY = config.MY_SQL_DICT_COLUMNS_QUERY  \n",
    "\n",
    "MS_SQL_PATH_DB = config.MS_SQL_PATH_DB\n",
    "MS_SQL_USER_DB = config.MS_SQL_USER_DB\n",
    "MS_SQL_PASSWORD_DB = config.MS_SQL_PASSWORD_DB\n",
    "MS_SQL_DB_COLUMNS_QUERY = config.MS_SQL_DB_COLUMNS_QUERY\n",
    "\n",
    "MS_SQL_PATH_DICT = config.MS_SQL_PATH_DICT\n",
    "MS_SQL_USER_DICT = config.MS_SQL_USER_DICT\n",
    "MS_SQL_PASSWORD_DICT = config.MS_SQL_PASSWORD_DICT\n",
    "MS_SQL_DICT_COLUMNS_QUERY = config.MS_SQL_DICT_COLUMNS_QUERY  \n",
    "\n",
    "SAP_HANA_DICT_COLUMNS_QUERY = config.SAP_HANA_DICT_COLUMNS_QUERY\n",
    "DICT_HOST = config.DICT_HOST\n",
    "DICT_PORT = config.DICT_PORT\n",
    "DICT_USER = config.DICT_USER\n",
    "DICT_PASSWORD = config.DICT_PASSWORD\n",
    "\n",
    "SAP_HANA_DB_COLUMNS_QUERY = config.SAP_HANA_DB_COLUMNS_QUERY\n",
    "DB_HOST = config.DB_HOST\n",
    "DB_PORT = config.DB_PORT\n",
    "DB_USER = config.DB_USER\n",
    "DB_PASSWORD = config.DB_PASSWORD\n",
    "\n",
    "SIZE = config.SIZE\n",
    "WINDOW = config.WINDOW\n",
    "MIN_COUNT = config.MIN_COUNT\n",
    "WORKERS = config.WORKERS\n",
    "EPOCHS = config.EPOCHS\n",
    "\n",
    "LOW_TO_HIGH_RANKING_ORDER = config.LOW_TO_HIGH_RANKING_ORDER\n",
    "NUM_OF_RESULTS = config.NUM_OF_RESULTS\n",
    "ACCURACY_PERCENTILE = config.ACCURACY_PERCENTILE\n",
    "\n",
    "\"\"\"GLOBAL CONFIG\"\"\"\n",
    "PATH_NON_TECH_DICT = config.PATH_NON_TECH_DICT\n",
    "PATH_TECH_DICT = config.PATH_TECH_DICT\n",
    "PATH_WORD2VEC_MODEL = config.PATH_WORD2VEC_MODEL\n",
    "PATH_WORD2VEC_DOCS = config.PATH_WORD2VEC_DOCS\n",
    "PATH_DB_SUBGROUPED = config.PATH_DB_SUBGROUPED\n",
    "PATH_OUTPUTS = config.PATH_OUTPUTS\n",
    "PATH_TEST_DATA = config.PATH_TEST_DATA\n",
    "\n",
    "LANGUAGE = config.LANGUAGE\n",
    "AUTOCORRECT_ON = config.AUTOCORRECT_ON\n",
    "STEMMED_WORDS = config.STEMMED_WORDS\n",
    "\n",
    "TECH_DICT_COLS = config.TECH_DICT_COLS\n",
    "TECH_DICT_TERM_COL = config.TECH_DICT_TERM_COL\n",
    "TECH_DICT_DEFN_COL = config.TECH_DICT_DEFN_COL\n",
    "\n",
    "REGEX_TECHNICAL_WORDS = config.REGEX_TECHNICAL_WORDS\n",
    "TECH_WORDS_REGEX = config.TECH_WORDS_REGEX\n",
    "\n",
    "CSV_PATH_DB = config.CSV_PATH_DB\n",
    "CSV_PATH_DICT = config.CSV_PATH_DICT\n",
    "\n",
    "XLSX_PATH_DB = config.XLSX_PATH_DB\n",
    "XLSX_PATH_DICT = config.XLSX_PATH_DICT\n",
    "\n",
    "SQL_PATH_DB = config.SQL_PATH_DB\n",
    "SQL_USER_DB = config.SQL_USER_DB\n",
    "SQL_PASSWORD_DB = config.SQL_PASSWORD_DB\n",
    "SQL_DB_COLUMNS_QUERY = config.SQL_DB_COLUMNS_QUERY\n",
    "\n",
    "SQL_PATH_DICT = config.SQL_PATH_DICT\n",
    "SQL_USER_DICT = config.SQL_USER_DICT\n",
    "SQL_PASSWORD_DICT = config.SQL_PASSWORD_DICT\n",
    "SQL_DICT_COLUMNS_QUERY = config.SQL_DICT_COLUMNS_QUERY  \n",
    "\n",
    "SAP_HANA_DICT_COLUMNS_QUERY = config.SAP_HANA_DICT_COLUMNS_QUERY\n",
    "DICT_HOST = config.DICT_HOST\n",
    "DICT_PORT = config.DICT_PORT\n",
    "DICT_USER = config.DICT_USER\n",
    "DICT_PASSWORD = config.DICT_PASSWORD\n",
    "\n",
    "SAP_HANA_DB_COLUMNS_QUERY = config.SAP_HANA_DB_COLUMNS_QUERY\n",
    "DB_HOST = config.DB_HOST\n",
    "DB_PORT = config.DB_PORT\n",
    "DB_USER = config.DB_USER\n",
    "DB_PASSWORD = config.DB_PASSWORD\n",
    "\n",
    "SIZE = config.SIZE\n",
    "WINDOW = config.WINDOW\n",
    "MIN_COUNT = config.MIN_COUNT\n",
    "WORKERS = config.WORKERS\n",
    "EPOCHS = config.EPOCHS\n",
    "\n",
    "LOW_TO_HIGH_RANKING_ORDER = config.LOW_TO_HIGH_RANKING_ORDER\n",
    "NUM_OF_RESULTS = config.NUM_OF_RESULTS\n",
    "ACCURACY_PERCENTILE = config.ACCURACY_PERCENTILE\n",
    "\n",
    "\"\"\"GLOBAL CONFIG\"\"\"\n",
    "PATH_NON_TECH_DICT = config.PATH_NON_TECH_DICT\n",
    "PATH_TECH_DICT = config.PATH_TECH_DICT\n",
    "PATH_WORD2VEC_MODEL = config.PATH_WORD2VEC_MODEL\n",
    "PATH_WORD2VEC_DOCS = config.PATH_WORD2VEC_DOCS\n",
    "PATH_DB_SUBGROUPED = config.PATH_DB_SUBGROUPED\n",
    "PATH_OUTPUTS = config.PATH_OUTPUTS\n",
    "PATH_TEST_DATA = config.PATH_TEST_DATA\n",
    "\n",
    "LANGUAGE = config.LANGUAGE\n",
    "AUTOCORRECT_ON = config.AUTOCORRECT_ON\n",
    "STEMMED_WORDS = config.STEMMED_WORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gensim model\"\"\"\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "\n",
    "\"\"\"NLTK natural language processing\"\"\"\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import pymssql\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from autocorrect import spell\n",
    "import pprint\n",
    "import pyhdb\n",
    "\n",
    "\"\"\"set some defaults when displaying tables\"\"\"\n",
    "pd.set_option('display.max_rows',50)\n",
    "pd.set_option('display.max_columns',50)\n",
    "\"\"\"Gensim model\"\"\"\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "\n",
    "\"\"\"NLTK natural language processing\"\"\"\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import pymssql\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from autocorrect import spell\n",
    "import pprint\n",
    "import pyhdb\n",
    "\n",
    "\"\"\"set some defaults when displaying tables\"\"\"\n",
    "pd.set_option('display.max_rows',50)\n",
    "pd.set_option('display.max_columns',50)\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(PATH_NON_TECH_DICT,\"rb\")\n",
    "non_technical_words=pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(PATH_TECH_DICT,\"rb\")\n",
    "technical_words=pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(PATH_DB_SUBGROUPED,\"rb\")\n",
    "db_subgrouped=pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_filters(stemming):\n",
    "    if(stemming == 1):\n",
    "        filters = [lambda x: x.lower(),strip_punctuation,remove_stopwords,strip_multiple_whitespaces,stem_text]\n",
    "    else:\n",
    "        filters = [lambda x: x.lower(),strip_punctuation,remove_stopwords,strip_multiple_whitespaces]\n",
    "    return filters\n",
    "\n",
    "CUSTOM_FILTERS = customize_filters(STEMMED_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_list=list(db_subgrouped.index)\n",
    "subgroup_num=len(subgroup_list)\n",
    "subgroup_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing \n",
    "\n",
    "def get_words(text):\n",
    "    return word_tokenize(text)\n",
    "        \n",
    "def get_autocorrected(word_list):\n",
    "    if(AUTOCORRECT_ON == 1):\n",
    "        for i in range(len(word_list)):\n",
    "            word = word_list[i]\n",
    "            if word not in technical_words.keys():\n",
    "                word_list[i] = spell(word)\n",
    "    return word_list\n",
    "        \n",
    "def preprocess_text(sentence):\n",
    "    word_list = get_words(sentence)\n",
    "    word_list = get_autocorrected(word_list)\n",
    "    sentence = ' '.join(word_list)\n",
    "    return preprocess_string(sentence, CUSTOM_FILTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dict()\n",
    "default=None\n",
    "\n",
    "# for each row in the indexed database, create a new (row,doc) into the dict \n",
    "for model in subgroup_list:\n",
    "    documents.setdefault(model,[])\n",
    "\n",
    "\"\"\" for each row in the indexed db\"\"\"\n",
    "for subgroup in subgroup_list:\n",
    "    \"\"\" convert the semantic row into a list of sentences \"\"\"\n",
    "    subgroup_semantic=list(db_subgrouped.loc[subgroup,'semantic'])\n",
    "    \"\"\" for each sentence convert it into a list of words \"\"\"\n",
    "    for sentence in subgroup_semantic:\n",
    "        sentence=preprocess_text(sentence)\n",
    "        model=subgroup\n",
    "        documents[model].append(sentence)\n",
    "        \"\"\" for each non-tech-word in the sentence, add its (list of synonyms) as sentence into the same document. This enhances the synonymous searching\"\"\"\n",
    "        for word in sentence:\n",
    "            if word in non_technical_words.keys():\n",
    "                documents[model].append(non_technical_words[word])\n",
    "            if word in technical_words.keys():\n",
    "                for description in technical_words[word]:\n",
    "                    sent = preprocess_text(description)\n",
    "                    documents[model].append(sent)\n",
    "                    \n",
    "#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "WORKERS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8cb2791a-e2e2-4f80-94b4-86423498232a",
    "_uuid": "d68145df8c268d833db8612e6d07557d8f28bffe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_num=dict()\n",
    "\"\"\"for each row,list of words in documents\"\"\"\n",
    "for k, v in documents.items():\n",
    "    \"\"\"create a model and set its parameters\"\"\"\n",
    "    model = gensim.models.Word2Vec(\n",
    "        v,\n",
    "        size=SIZE,\n",
    "        window=WINDOW,\n",
    "        min_count=MIN_COUNT,\n",
    "        workers=WORKERS,\n",
    "        hs=1,\n",
    "        negative=1)\n",
    "    \"\"\"train the model on the list of words\"\"\"\n",
    "    model.train(v, total_examples=len(v), epochs=EPOCHS)\n",
    "    \"\"\"you can see the vocabulary of the model by\"\"\"\n",
    "    #print(model.wv.vocab)\n",
    "    \"\"\"insert the (row,model) into the dictionary\"\"\"\n",
    "    model_num[k]=model\n",
    "\n",
    "\"\"\"model_num[row]=model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(PATH_WORD2VEC_DOCS,\"wb\")\n",
    "pickle.dump(documents,f)\n",
    "f.close()\n",
    "\n",
    "f = open(PATH_WORD2VEC_MODEL,\"wb\")\n",
    "pickle.dump(model_num,f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
